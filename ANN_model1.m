% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created 01-Jul-2019 11:23:43
%
% This script assumes these variables are defined:
%
%   data - input data.
%   data_1 - target data.

%Clear old stuff and create folder named networks if it doesn't 
%already  exist


clear all; 
fclose all; 
clc;



if isdir('networks')==0
    mkdir('networks');
end

Inputs=dlmread('Inputs.txt', '\t', 1, 0);  %input data
Targets=dlmread('Targets.txt', '\t', 1, 0);  %target data
x = Inputs';
t = Targets';

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.

% trainlm----------------------	Levenberg-Marquardt -Val
% trainbr	Bayesian Regularization - N0 Val
% trainbfg	BFGS Quasi-Newton - Val
% trainrp	Resilient Backpropagation - Val
% trainscg	Scaled Conjugate Gradient - val
% traincgb	Conjugate Gradient with Powell/Beale Restarts - val
% traincgf	Fletcher-Powell Conjugate Gradient - val
% traincgp	Polak-Ribiére Conjugate Gradient - val
% trainoss	One Step Secant - val
% traingdx	Variable Learning Rate Gradient Descent - val
% traingdm	Gradient Descent with Momentum - val
% traingd	Gradient Descent - val
% traingda                   - val

% train_algorithm= ["trainlm"; "trainbr"];


% for j= 1:2
    %trainFcn = train_algorithm(j);
    
    % Create a Fitting Network
trainFcn = 'trainbr';
n_hlayer = 20;
        
for i=1:n_hlayer  %vary number of hidden layer neurons from 1 to 100
    tic
    
    hiddenLayerSize = i;
    % net = fitnet(hiddenLayerSize,trainFcn);
    net = feedforwardnet(hiddenLayerSize,trainFcn);
    %net.layers{1}.transferFcn='logsig';
    %net.layers{2}.transferFcn='purelin';
    %net.layers{3}.transferFcn='purelin';    
    % Choose Input and Output Pre/Post-Processing Functions
    % For a list of all processing functions type: help nnprocess
    net.input.processFcns = {'removeconstantrows','mapminmax'};
    net.output.processFcns = {'removeconstantrows','mapminmax'};
        
    % Setup Division of Data for Training, Validation, Testing
    % For a list of all data division functions type: help nndivision
    net.divideFcn = 'dividerand';  % Divide data randomly
    net.divideMode = 'sample';  % Divide up every sample
    net.divideParam.trainRatio = 85/100;
%     net.divideParam.valRatio = 15/100;
    net.divideParam.testRatio = 15/100;
        
    % Choose Plot Functions
    % For a list of all plot functions type: help nnplot
    net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
        'plotregression', 'plotfit'};
    % Train the Network
    [net,tr] = train(net,x,t);
    
    % performance = perform(net,t,y);
    output_tr = net(x(:,tr.trainInd));
    output_te = net(x(:,tr.testInd));
        
    e_tr = output_tr-t(tr.trainInd);
    e_te = output_te-t(tr.testInd);
       
    maetr(i) = mean(abs(output_tr-t(tr.trainInd)));
    maete(i) = mean(abs(output_te-t(tr.testInd)));
    
    mretr(i) = mean(abs(output_tr-t(tr.trainInd))/output_tr);
    mrete(i) = mean(abs(output_te-t(tr.testInd))/output_te);
        
    msetr(i) = mean((output_tr-t(tr.trainInd)).^2);
    msete(i) = mean((output_te-t(tr.testInd)).^2);
        
    rmsetr(i) = sqrt(mean((output_tr-t(tr.trainInd)).^2)); %RMSE for 70% random trainig data
    rmsete(i) = sqrt(mean((output_te-t(tr.testInd)).^2)); %RMSE for 15% random test data
        
    R_tr(i) = regression(t(tr.trainInd), output_tr);
    R_te(i) = regression(t(tr.testInd), output_te);
        
    % This is according to the definition of a website:
    % (https://towardsdatascience.com/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0)
    %msetr_b = mean((t(tr.trainInd) - mean(t(tr.trainInd))).^2);
    %msete_b = mean((t(tr.testInd)- mean(t(tr.testInd))).^2);
        
    %R2_tr = 1 - (msetr/msetr_b);
    %R2_te = 1 - (msete/msete_b);
    save(['networks\net' num2str(i)]);  %save the network in networks folder
    elapsed_time (i) = toc;
end



%Save the RMSEs
fid=fopen('rmse.txt', 'wt');
fprintf(fid, 'Nh\t rmsetr\t rmsete\t msetr\t msete\t maetr\t maete\t mretr\t mrete\t R_tr\t R_te\t elapsed_time\n');
fprintf(fid, '%4.0f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\n', [1:n_hlayer; rmsetr; rmsete; msetr; msete; maetr; maete; mretr; mrete; R_tr; R_te; elapsed_time]);
fclose all;
     
%Plot the RMSEs
plot(1:n_hlayer, rmsetr, 'b*-'); hold on; plot(1:n_hlayer, rmsete, 'ro-');
legend('Random', 'final-test'); xlabel('Number of hidden layer neurons');
ylabel('RMSE'); 
   

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotregression(t,y)
%figure, plotfit(net,x,t)



