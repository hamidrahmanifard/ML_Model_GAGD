% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created 01-Jul-2019 11:23:43
%
% This script assumes these variables are defined:
%
%   data - input data.
%   data_1 - target data.

%Clear old stuff and create folder named networks if it doesn't 
%already  exist


clear all; fclose all; clc;



if isdir('networks')==0
    mkdir('networks');
end

Inputs=dlmread('Inputs.txt', '\t', 1, 0);  %input data
Targets=dlmread('Targets.txt', '\t', 1, 0);  %target data
x = Inputs';
t = Targets';

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.

% trainlm----------------------	Levenberg-Marquardt
% trainbr	Bayesian Regularization
% trainbfg	BFGS Quasi-Newton
% trainrp	Resilient Backpropagation
% trainscg	Scaled Conjugate Gradient
% traincgb	Conjugate Gradient with Powell/Beale Restarts
% traincgf	Fletcher-Powell Conjugate Gradient
% traincgp	Polak-Ribiére Conjugate Gradient
% trainoss	One Step Secant
% traingdx	Variable Learning Rate Gradient Descent
% traingdm	Gradient Descent with Momentum
% traingd	Gradient Descent
% traingda

% train_algorithm= ["trainlm"; "trainbr"];


% for j= 1:2
    %trainFcn = train_algorithm(j);
    
    % Create a Fitting Network
trainFcn = 'trainlm';
n_hlayer = 20;

fid=fopen('rmse.txt', 'wt');
fprintf(fid, 'Nh\t rmsetr\t rmsete\t msetr\t msete\t maetr\t maete\t mretr\t mrete\t R_tr\t R_te\t elapsed_time\n');
    
for i=1:n_hlayer  %vary number of hidden layer neurons from 1 to 100

    for j = 1:n_hlayer
   tic 
    
    hiddenLayerSize1 = i;
    hiddenLayerSize2 = j;

    % net = fitnet(hiddenLayerSize,trainFcn);
    net = feedforwardnet([hiddenLayerSize1,hiddenLayerSize2],trainFcn);
    %net.layers{1}.transferFcn='logsig';
    %net.layers{2}.transferFcn='purelin';
    %net.layers{3}.transferFcn='purelin';    
    % Choose Input and Output Pre/Post-Processing Functions
    % For a list of all processing functions type: help nnprocess
    net.input.processFcns = {'removeconstantrows','mapminmax'};
    net.output.processFcns = {'removeconstantrows','mapminmax'};
        
    % Setup Division of Data for Training, Validation, Testing
    % For a list of all data division functions type: help nndivision
    net.divideFcn = 'dividerand';  % Divide data randomly
    net.divideMode = 'sample';  % Divide up every sample
    net.divideParam.trainRatio = 70/100;
    net.divideParam.valRatio = 15/100;
    net.divideParam.testRatio = 15/100;
        
    % Choose Plot Functions
    % For a list of all plot functions type: help nnplot
    net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
        'plotregression', 'plotfit'};
    % Train the Network
    [net,tr] = train(net,x,t);
    
    % performance = perform(net,t,y);
    output_tr = net(x(:,tr.trainInd));
    output_te = net(x(:,tr.testInd));
        
    e_tr = output_tr-t(tr.trainInd);
    e_te = output_te-t(tr.testInd);
    
    a1 = num2str(i);
    a2 = num2str(j);
    c1 = strcat(a1,'-', a2);
    c2 = strcat(a1,a2);
    layer(j) = str2double(c2);
       
    maetr(j) = mean(abs(output_tr-t(tr.trainInd)));
    maete(j) = mean(abs(output_te-t(tr.testInd)));
    
    mretr(j) = mean(abs(output_tr-t(tr.trainInd))/output_tr);
    mrete(j) = mean(abs(output_te-t(tr.testInd))/output_te);
    
    msetr(j) = mean((output_tr-t(tr.trainInd)).^2);
    msete(j) = mean((output_te-t(tr.testInd)).^2);
        
    rmsetr(j) = sqrt(mean((output_tr-t(tr.trainInd)).^2)); %RMSE for 70% random trainig data
    rmsete(j) = sqrt(mean((output_te-t(tr.testInd)).^2)); %RMSE for 15% random test data
        
    R_tr(j) = regression(t(tr.trainInd), output_tr);
    R_te(j) = regression(t(tr.testInd), output_te);
        
    % This is according to the definition of a website:
    % (https://towardsdatascience.com/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0)
    %msetr_b = mean((t(tr.trainInd) - mean(t(tr.trainInd))).^2);
    %msete_b = mean((t(tr.testInd)- mean(t(tr.testInd))).^2);
        
    %R2_tr = 1 - (msetr/msetr_b);
    %R2_te = 1 - (msete/msete_b);
    save(['networks\net' c1]);  %save the network in networks folder
    elapsed_time (j) = toc;
          
    end
    
    %Save the RMSEs
    fprintf(fid, '%4.0f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\n', [layer; rmsetr; rmsete; msetr; msete; maetr; maete; mretr; mrete; R_tr; R_te; elapsed_time]);
end

fclose all;


% %Save the RMSEs
% fid=fopen('rmse.txt', 'wt');
% fprintf(fid, 'Nh\t rmsetr\t rmsete\t msetr\t msete\t maetr\t maete\t R_tr\t R_te\t elapsed_time\n');
% fprintf(fid, '%4.0f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\t %f\n', [layer; rmsetr; rmsete; msetr; msete; maetr; maete; R_tr; R_te; elapsed_time]);
% fclose all;
%      
% %Plot the RMSEs
% plot(1:n_hlayer, rmsetr, 'b*-'); hold on; plot(1:n_hlayer, rmsete, 'ro-');
% legend('Random', 'final-test'); xlabel('Number of hidden layer neurons');
% ylabel('RMSE'); 
%    

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotregression(t,y)
%figure, plotfit(net,x,t)



